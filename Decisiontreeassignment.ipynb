{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOugFvThQk7Tescfv18XBZ7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nishil2009/Data-anlaytics-course/blob/main/Decisiontreeassignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "-Decision Tree is a supervised machine learning algorithm that can be used for both classification and regression tasks. It uses a tree-like model of decisions and their possible consequences. Each internal node represents a test on a feature (e.g., \"Is a patient's temperature > 99Â°F?\"), each branch represents the outcome of the test, and each leaf node represents a class label (in classification) or a predicted value (in regression). The path from the root to a leaf represents the sequence of decisions that leads to the final classification.\n",
        "\n",
        "-In classification, the algorithm works by recursively splitting the dataset into subsets based on the value of a single feature. The goal is to create subsets that are as \"pure\" as possible, meaning they contain data points belonging to a single class. The split that results in the greatest reduction in impurity is chosen at each step. This process continues until a stopping criterion is met, such as the nodes being pure or a maximum depth being reached.\n",
        "\n",
        "2. Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Gini Impurity measures how often a randomly chosen element from a set would be incorrectly labeled if it were labeled randomly according to the distribution of labels in the subset. A Gini impurity of 0 means the set is perfectly pure (all elements belong to the same class), while a value of 1 means the set is completely mixed.\n",
        "\n",
        "Entropy is another measure of impurity that quantifies the randomness or uncertainty in a set of data. A pure set has an entropy of 0, while a perfectly random or equally mixed set has the highest entropy.\n",
        "\n",
        "\n",
        "1. What is a Decision Tree, and how does it work in the context of classification?\n",
        "A\n",
        "\n",
        "Decision Tree is a supervised machine learning algorithm that can be used for both classification and regression tasks. It uses a tree-like model of decisions and their possible consequences. Each internal node represents a test on a feature (e.g., \"Is a patient's temperature > 99Â°F?\"), each branch represents the outcome of the test, and each leaf node represents a class label (in classification) or a predicted value (in regression). The path from the root to a leaf represents the sequence of decisions that leads to the final classification.\n",
        "\n",
        "In\n",
        "\n",
        "classification, the algorithm works by recursively splitting the dataset into subsets based on the value of a single feature. The goal is to create subsets that are as \"pure\" as possible, meaning they contain data points belonging to a single class. The split that results in the greatest reduction in impurity is chosen at each step. This process continues until a stopping criterion is met, such as the nodes being pure or a maximum depth being reached.\n",
        "\n",
        "2. Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Gini Impurity measures how often a randomly chosen element from a set would be incorrectly labeled if it were labeled randomly according to the distribution of labels in the subset. A Gini impurity of 0 means the set is perfectly pure (all elements belong to the same class), while a value of 1 means the set is completely mixed.\n",
        "\n",
        "\n",
        "Entropy is another measure of impurity that quantifies the randomness or uncertainty in a set of data. A pure set has an entropy of 0, while a perfectly random or equally mixed set has the highest entropy.\n",
        "\n",
        "These measures impact the splits in a Decision Tree by helping the algorithm decide which feature and threshold to use for a split. The algorithm aims to find the split that results in the lowest possible Gini impurity or entropy in the resulting child nodes. The feature that provides the greatest reduction in impurity (known as Information Gain) is selected as the best splitting criterion for a node.\n",
        "\n",
        "3. What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "-Pruning is a technique used to avoid overfitting in Decision Trees by reducing the size of the tree.\n",
        "\n",
        "\n",
        "-Pre-Pruning involves stopping the tree-building process early before it fully grows. This is done by setting a stopping condition, such as a maximum depth for the tree, a minimum number of samples required to split a node, or a minimum impurity decrease needed for a split. A practical advantage of pre-pruning is that it's computationally faster and can prevent the model from becoming overly complex from the start, making it easier to interpret.\n",
        "\n",
        "\n",
        "-Post-Pruning involves growing the full, unpruned tree first and then removing or merging branches that provide little value or that are based on small subsets of data. This is often done using a validation set to evaluate the performance of the pruned tree. A practical advantage of post-pruning is that it can lead to a more effective final model since it considers the full tree structure before making any cuts, potentially identifying subtrees that are important but might have been pruned in a pre-pruning scenario.\n",
        "\n",
        "4. What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "Information Gain is the measure of the reduction in entropy or impurity after a dataset is split on an attribute. It quantifies how much a feature contributes to making the data more homogeneous with respect to the class labels.\n",
        "\n",
        "Information Gain is important for choosing the best split because it's the primary criterion used to select which feature to split on at each node of the tree. The algorithm calculates the information gain for each available feature and selects the one that provides the highest gain. This ensures that at each step, the tree is being built in a way that most effectively reduces uncertainty and moves towards creating pure leaf nodes, leading to a more efficient and accurate model.\n",
        "\n",
        "5. What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "\n",
        "Common Real-World Applications\n",
        "\n",
        "Customer Relationship Management (CRM): Predicting customer churn or identifying potential customers who are likely to make a purchase.\n",
        "\n",
        "\n",
        "Healthcare: Diagnosing diseases based on patient symptoms and medical test results.\n",
        "\n",
        "\n",
        "Finance: Credit risk assessment to predict loan defaults.\n",
        "\n",
        "\n",
        "E-commerce: Recommender systems to suggest products to users.\n",
        "\n",
        "Advantages and Limitations\n",
        "\n",
        "Advantages:\n",
        "Easy to Understand and Interpret: The logic of a Decision Tree can be easily visualized and explained to non-technical stakeholders, as it mimics human decision-making.\n",
        "\n",
        "Requires Little Data Preparation: Decision Trees don't require data normalization and can handle both numerical and categorical data.\n",
        "\n",
        "Flexible: They can be used for both classification and regression tasks.\n",
        "\n",
        "\n",
        "Limitations:\n",
        "Overfitting: Decision Trees can easily overfit the training data, leading to a model that performs poorly on new, unseen data.\n",
        "\n",
        "Instability: A small change in the data can result in a completely different tree structure.\n",
        "\n",
        "Bias: They can be biased towards features with many levels, which can lead to skewed results.\n",
        "\n"
      ],
      "metadata": {
        "id": "fRubNkr2W2p0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#6.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier with the Gini criterion\n",
        "dt_classifier = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print feature importances\n",
        "feature_importances = dt_classifier.feature_importances_\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, feature_importances):\n",
        "    print(f\"  {feature}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsM0LPYAYnzp",
        "outputId": "d954b9aa-43dc-4e69-d551-aa31edcc52c1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0000\n",
            "Feature Importances:\n",
            "  sepal length (cm): 0.0000\n",
            "  sepal width (cm): 0.0191\n",
            "  petal length (cm): 0.8933\n",
            "  petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier with max_depth=3\n",
        "dt_pruned = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "dt_pruned.fit(X_train, y_train)\n",
        "y_pred_pruned = dt_pruned.predict(X_test)\n",
        "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
        "\n",
        "# Train a fully-grown Decision Tree Classifier (default max_depth=None)\n",
        "dt_full = DecisionTreeClassifier(random_state=42)\n",
        "dt_full.fit(X_train, y_train)\n",
        "y_pred_full = dt_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print the accuracies for comparison\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_pruned:.4f}\")\n",
        "print(f\"Accuracy with a fully-grown tree: {accuracy_full:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASbFVzfPY0if",
        "outputId": "164e33d2-b073-4a42-b05e-858e8961fb22"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.0000\n",
            "Accuracy with a fully-grown tree: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "dt_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate MSE\n",
        "y_pred = dt_regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "\n",
        "# Print feature importances\n",
        "feature_importances = dt_regressor.feature_importances_\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(housing.feature_names, feature_importances):\n",
        "    print(f\"  {feature}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSFG67cdY5wM",
        "outputId": "b852467c-c1c0-45a4-c777-9610062cb6de"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.5280\n",
            "Feature Importances:\n",
            "  MedInc: 0.5235\n",
            "  HouseAge: 0.0521\n",
            "  AveRooms: 0.0494\n",
            "  AveBedrms: 0.0250\n",
            "  Population: 0.0322\n",
            "  AveOccup: 0.1390\n",
            "  Latitude: 0.0900\n",
            "  Longitude: 0.0888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9.\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10, 15]\n",
        "}\n",
        "\n",
        "# Initialize the Decision Tree classifier\n",
        "dtc = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV with the classifier and parameter grid\n",
        "grid_search = GridSearchCV(estimator=dtc, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Get the best model and evaluate its accuracy on the test set\n",
        "best_dtc = grid_search.best_estimator_\n",
        "y_pred_best = best_dtc.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred_best)\n",
        "\n",
        "print(f\"Accuracy with best parameters: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxYTN0S5Y-ff",
        "outputId": "8be42cba-5cd8-4657-8637-b2e1adb8006a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Accuracy with best parameters: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Imagine you're working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to handle the missing values, encode the categorical features, train a Decision Tree model, tune its hyperparameters, evaluate its performance, and describe what business value this model could provide in the real-world setting.\n",
        "\n",
        "\n",
        "Step-by-Step Process ðŸ“ˆ\n",
        "1. Handle Missing Values:\n",
        "\n",
        "First, I'd analyze the dataset to understand the nature and extent of the missing values.\n",
        "\n",
        "For numerical features with a small percentage of missing data, I would use imputation. I might fill the missing values with the mean or median of the feature, as these methods are less sensitive to outliers.\n",
        "\n",
        "For categorical features with missing values, I would use a different imputation strategy, such as filling them with the mode (most frequent category) or creating a new category like \"Unknown\" to represent the missing data.\n",
        "\n",
        "If a feature has a very high percentage of missing values (e.g., >50%), it might be more prudent to drop the entire column, as it might not provide enough predictive power.\n",
        "\n",
        "2. Encode Categorical Features:\n",
        "\n",
        "Decision Trees cannot directly handle text data. I'd need to convert the categorical features (like patient's gender or blood type) into a numerical format.\n",
        "\n",
        "One-Hot Encoding would be a suitable method for nominal categorical variables (like blood type) to avoid introducing a false sense of order. This creates new binary columns for each category.\n",
        "\n",
        "For ordinal categorical variables (like disease severity: 'mild', 'moderate', 'severe'), Label Encoding could be used, assigning an integer to each category while preserving their order.\n",
        "\n",
        "3. Train a Decision Tree Model:\n",
        "\n",
        "After preprocessing, I would split the data into a training set and a testing set. The training set is used to train the model, while the testing set is reserved to evaluate its performance on unseen data.\n",
        "\n",
        "I would then initialize a DecisionTreeClassifier and fit it to the training data. For a large dataset, I would set a random_state for reproducibility and a class_weight parameter to handle any class imbalance, which is common in disease prediction datasets.\n",
        "\n",
        "4. Tune its Hyperparameters:\n",
        "\n",
        "To prevent overfitting and find the optimal model, I would use GridSearchCV or RandomizedSearchCV to tune the hyperparameters.\n",
        "\n",
        "The key parameters to tune for a Decision Tree would be max_depth (to control the tree's complexity), min_samples_leaf (to ensure splits are not based on very small groups), and criterion (Gini or Entropy).\n",
        "\n",
        "GridSearchCV would systematically test different combinations of these parameters to find the set that yields the best performance, as measured by a chosen metric (e.g., accuracy, F1-score).\n",
        "\n",
        "5. Evaluate its Performance:\n",
        "\n",
        "After tuning, I would evaluate the final model on the held-out test set.\n",
        "\n",
        "Since this is a classification task, I would use metrics beyond simple accuracy, especially if the dataset is imbalanced. A confusion matrix would show true positives, false positives, true negatives, and false negatives.\n",
        "\n",
        "Precision, Recall, and the F1-score would provide a more complete picture of the model's performance. The Receiver Operating Characteristic (ROC) curve and Area Under the Curve (AUC) would also be useful to assess the model's ability to distinguish between classes.\n",
        "\n",
        "Business Value -\n",
        "This model could provide significant real-world business value for a healthcare company.\n",
        "\n",
        "Early Diagnosis: The model could help doctors in clinics quickly identify high-risk patients who should be prioritized for further diagnostic tests. This could lead to earlier treatment and better patient outcomes.\n",
        "\n",
        "Resource Allocation: By predicting disease likelihood, the company could optimize the allocation of resources, such as medical staff, hospital beds, and specific treatments, to areas where they are most needed.\n",
        "\n",
        "Preventive Care: The model could identify the key features (symptoms, lab results, etc.) that are most predictive of the disease. The company could use these insights to develop targeted preventive care programs and educational campaigns to reduce the disease's incidence."
      ],
      "metadata": {
        "id": "DEihlcAjaEJJ"
      }
    }
  ]
}